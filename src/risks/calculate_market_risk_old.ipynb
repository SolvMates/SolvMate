{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3643ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read the configuration table and filter for WORKSHEET = 'MarketR'\n",
    "data_id = spark.sql(\"SELECT * FROM default.data_id WHERE WORKSHEET = 'MarketR'\")\n",
    "\n",
    "# Step 2: Read the 'MarketR' sheet from the unstructured Excel file\n",
    "#excel_file_path = \"/Volumes/workspace/default/input/MarketR.xlsx\" # for test\n",
    "\n",
    "#excel_file_path = dbutils.widgets.get(\"input_path\") # Parameter set by workflow\n",
    "#df_excel = pd.read_excel(excel_file_path, sheet_name='MarketR', header=None)\n",
    "\n",
    "# Step 3: Create a function to extract values based on RC_CODE\n",
    "def get_cell_value(rc_code):\n",
    "    if rc_code:\n",
    "        # Extract the row and column from RC_CODE\n",
    "        row_num = int(rc_code.split('C')[0][1:])  # Get the number after 'R'\n",
    "        col_num = int(rc_code.split('C')[1])      # Get the number after 'C'\n",
    "        \n",
    "        # Retrieve the value from the DataFrame\n",
    "        return df_excel.iat[row_num - 1, col_num - 1] if (0 <= row_num - 1 < len(df_excel)) and (0 <= col_num - 1 < len(df_excel.columns)) else None\n",
    "    return None\n",
    "\n",
    "# Register the UDF\n",
    "get_cell_value_udf = udf(get_cell_value, StringType())\n",
    "\n",
    "# Step 4: Create a new column 'VALUE' in data_id\n",
    "data_id = data_id.withColumn(\"VALUE\", get_cell_value_udf(data_id[\"RC_CODE\"]))\n",
    "\n",
    "# Step 5: Store the updated DataFrame as a new table\n",
    "data_id.write.mode(\"overwrite\").saveAsTable(\"default.data_id_updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d5d20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e6eba95",
   "metadata": {},
   "source": [
    "%md\n",
    "Prompt, with which the following code was drafted\n",
    "The following table aggregation_tree_market contains input for a market SCR calculation (without currency and concentration risk) in Solvency II in form of an aggregation tree structure, where column _NODE_ID\n",
    "contains all nodes of the tree, column _PARENT_NODE_ID contains their respective parent node and AGGREGATION_METHOD_CD contains either the value \"external\", which means that the input of the value is provided externally via a variable (column DATA_ID) in table \"data_id_updated\", or one of the following aggregation methods:\n",
    "-  \"sum\": The value for this node is calculated as a sum of all child nodes\n",
    "-  \"max\": The value for this node is calculated as the maximum of all child nodes\n",
    "-  \"correlated\": The value for this node is calculated with the help of a correlation matrix. The name of the matrix to be used in the aggregation is given in column _MATRIX_ID. The values of all correlation matrices are stored in table default.correlation_matrix, which consists of the following columns: \n",
    "-- CORRELATION_MATRIX_ID: ID of correlation matrix, which is referenced in aggregation_tree_market._MATRIX_ID\n",
    "-- VAR1_NM: ID of variable 1\n",
    "-- VAR2_NM: ID of variable 2\n",
    "-- CORRELATION_VALUE_NO: Correlation value\n",
    "The resulting aggregation logic using \"correlated is: sqrt(sum(Corr_i,j*Node_i*Node_j)) over all child nodes of the respective node.\n",
    "- \"dnav\": All child nodes of a node with aggregation method \"dnav\" are either assets or liabilities. This information is specified in column \"BS_TYPE\" with \"asset\" for assets and \"liab\" for liabilities. Each asset or liability has a scenario that is specified in column \"SCENARIO\": \"BC\" is the base case and \"SH\" is the shocked scenario. With this information, the aggregation logic for \"dnav\" is: (Sum of all base case assets - sum of all base case liabilities) - ((Sum of all shocked assets - sum of all shocked liabilities)).\n",
    "- \"max_scen\": Does the same as \"max\" for now.\n",
    "\n",
    "Can you please write a method, that takes the input \"aggregation_tree_id\" and does the following steps: \n",
    "1. Read in the tables \"aggregation_tree\" and \"data_id_updated\" as data frames.\n",
    "2. Add the column \"VALUE\" to the data frame \"aggregation_tree\".\n",
    "3. For the specified \"aggregation_tree_id\", read in all values for all nodes with \"_AGGREGATION_METHOD_CD\" = \"external\" from data frame \"data_id_updated\", where _NODE_ID = DATA_ID in \"data_id_updated\".\n",
    "4. Aggregate all other values using their aggregation method, which is definded in column \"AGGREGATION_METHOD_CD\".\n",
    "5. Save the results in a new table in schema \"default\" with name \"aggregation_tree_market_enriched\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sqrt, sum as spark_sum, max as spark_max, when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MarketSCRAggregation\").getOrCreate()\n",
    "\n",
    "def aggregate_tree(aggregation_tree_id):\n",
    "    \"\"\"\n",
    "    Aggregates market SCR values based on the specified aggregation tree ID.\n",
    "\n",
    "    Parameters:\n",
    "    aggregation_tree_id (str): The ID of the aggregation tree to be used for aggregation.\n",
    "\n",
    "    Steps:\n",
    "    1. Reads in the aggregation tree and data ID updated tables using Spark SQL.\n",
    "    2. Initializes the \"VALUE\" column in the aggregation tree DataFrame to 0.\n",
    "    3. Reads in values for nodes with \"_AGGREGATION_METHOD_CD\" = \"external\" and updates the \"VALUE\" column.\n",
    "    4. Aggregates values for other nodes based on their aggregation method.\n",
    "        - Supported methods: 'sum', 'max', 'correlated', 'dnav', 'max_scen'.\n",
    "    5. Saves the results in a new table \"aggregation_tree_market_enriched\" in the \"default\" schema.\n",
    "\n",
    "    Example usage:\n",
    "    aggregate_market_scr('MARKET_INT')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Read in the tables using Spark SQL\n",
    "    aggregation_tree = spark.sql(f'SELECT * FROM default.aggregation_tree_market WHERE AGGREGATION_TREE_ID = \"{aggregation_tree_id}\"')\n",
    "    data_id_updated = spark.sql('SELECT * FROM default.data_id_updated')\n",
    "\n",
    "    # Step 2: Add the column \"VALUE\" to the data frame \"aggregation_tree\" and initialize to 0\n",
    "    aggregation_tree = aggregation_tree.withColumn(\"VALUE\", when(col(\"_NODE_ID\").isNotNull(), 0).cast(\"double\"))\n",
    "\n",
    "    # Step 3: Read in all values for nodes with \"_AGGREGATION_METHOD_CD\" = \"external\"\n",
    "    external_nodes = aggregation_tree.filter(col(\"_AGGREGATION_METHOD_CD\") == \"external\")\n",
    "    external_node_ids = [row[\"_NODE_ID\"] for row in external_nodes.collect()]\n",
    "    external_values = data_id_updated.filter(data_id_updated[\"DATA_ID\"].isin(external_node_ids))\n",
    "\n",
    "    for row in external_nodes.collect():\n",
    "        value_row = external_values.filter(external_values[\"DATA_ID\"] == row[\"_NODE_ID\"]).first()\n",
    "        if value_row:\n",
    "            aggregation_tree = aggregation_tree.withColumn(\"VALUE\", \n",
    "                when(col(\"_NODE_ID\") == row[\"_NODE_ID\"], value_row[\"VALUE\"]).otherwise(col(\"VALUE\")))\n",
    "\n",
    "    # Step 4: Aggregate all other values using their aggregation method\n",
    "    def aggregate_node(node_id):\n",
    "        node = aggregation_tree.filter(col(\"_NODE_ID\") == node_id).first()\n",
    "        if not node:\n",
    "            return None\n",
    "\n",
    "        method = node[\"_AGGREGATION_METHOD_CD\"]\n",
    "        children = aggregation_tree.filter(col(\"_PARENT_NODE_ID\") == node_id)\n",
    "\n",
    "        if method == 'sum':\n",
    "            return children.agg(spark_sum(\"VALUE\")).first()[0]\n",
    "        elif method == 'max':\n",
    "            return children.agg(spark_max(\"VALUE\")).first()[0]\n",
    "        elif method == 'correlated':\n",
    "            matrix_id = node[\"_MATRIX_ID\"]\n",
    "            correlation_matrix = spark.sql(f'SELECT * FROM default.correlation_matrix WHERE CORRELATION_MATRIX_ID = \"{matrix_id}\"')\n",
    "            total = 0\n",
    "            child_ids = [row[\"_NODE_ID\"] for row in children.collect()]\n",
    "            for i in range(len(child_ids)):\n",
    "                for j in range(len(child_ids)):\n",
    "                    if i != j:\n",
    "                        corr_value = correlation_matrix.filter((correlation_matrix[\"VAR1_NM\"] == child_ids[i]) & \n",
    "                                                               (correlation_matrix[\"VAR2_NM\"] == child_ids[j])).first()\n",
    "                        if corr_value:\n",
    "                            total += corr_value[\"CORRELATION_VALUE_NO\"] * (children.filter(col(\"_NODE_ID\") == child_ids[i]).select(\"VALUE\").first()[0] * \n",
    "                                                                            children.filter(col(\"_NODE_ID\") == child_ids[j]).select(\"VALUE\").first()[0])\n",
    "            return sqrt(total)\n",
    "        elif method == 'dnav':\n",
    "            base_case_assets = children.filter((col(\"BS_TYPE\") == 'asset') & (col(\"SCENARIO\") == 'BC')).agg(spark_sum(\"VALUE\")).first()[0]\n",
    "            base_case_liabilities = children.filter((col(\"BS_TYPE\") == 'liab') & (col(\"SCENARIO\") == 'BC')).agg(spark_sum(\"VALUE\")).first()[0]\n",
    "            shocked_assets = children.filter((col(\"BS_TYPE\") == 'asset') & (col(\"SCENARIO\") == 'SH')).agg(spark_sum(\"VALUE\")).first()[0]\n",
    "            shocked_liabilities = children.filter((col(\"BS_TYPE\") == 'liab') & (col(\"SCENARIO\") == 'SH')).agg(spark_sum(\"VALUE\")).first()[0]\n",
    "            return (base_case_assets - base_case_liabilities) - (shocked_assets - shocked_liabilities)\n",
    "        elif method == 'max_scen':\n",
    "            return children.agg(spark_max(\"VALUE\")).first()[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Apply aggregation recursively for all nodes\n",
    "    node_ids = [row[\"_NODE_ID\"] for row in aggregation_tree.collect()]\n",
    "    for node_id in node_ids:\n",
    "        if aggregation_tree.filter(col(\"_NODE_ID\") == node_id).select(\"VALUE\").first()[0] is None:\n",
    "            value = aggregate_node(node_id)\n",
    "            aggregation_tree = aggregation_tree.withColumn(\"VALUE\", \n",
    "                when(col(\"_NODE_ID\") == node_id, value).otherwise(col(\"VALUE\")))\n",
    "\n",
    "    # Step 5: Save the results in a new table in schema \"default\" with name \"aggregation_tree_market_enriched\"\n",
    "    aggregation_tree.select(\"_NODE_ID\", \"VALUE\").write.mode(\"overwrite\").saveAsTable(\"default.aggregation_tree_market_enriched\")\n",
    "\n",
    "# Example usage\n",
    "aggregate_tree('MARKET_INT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b464e71",
   "metadata": {},
   "source": [
    "The following code takes the aggregated values from before and writer ist into an output file, using the logic in the output_template.xlsx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7432279",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Databricks notebook source\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import openpyxl\n",
    "import string\n",
    "import pyspark.pandas as ps\n",
    "from shutil import move\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Define volume, folder, and file details.\n",
    "catalog            = 'workspace'\n",
    "schema             = 'default'\n",
    "volume             = 'output'\n",
    "volume_path        = f\"/Volumes/{catalog}/{schema}/{volume}\" # /Volumes/main/default/my-volume\n",
    "\n",
    "#RUN_ID = \"001\"  # For test purposes; If the script is run by workflow, this is set as a parameter\n",
    "RUN_ID = dbutils.widgets.get(\"run_id\")\n",
    "volume_folder   = f\"output_{RUN_ID}\"\n",
    "volume_folder_path = f\"{volume_path}/{volume_folder}\" # /Volumes/main/default/my-volume/my-folder\n",
    "volume_file        = f\"Output_{RUN_ID}.xlsx\"\n",
    "volume_file_path   = f\"{volume_folder_path}/{volume_file}\" # /Volumes/main/default/\n",
    "\n",
    "# Create an empty folder in a volume.\n",
    "w.files.create_directory(volume_folder_path)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "\n",
    "# Read in the tables as data frames\n",
    "Output_Enriched2_df = spark.table(\"default.Output_Enriched2\")\n",
    "\n",
    "# Load template\n",
    "template_file = f\"{volume_path}/Output_Template.xlsx\"\n",
    "template = openpyxl.load_workbook(template_file)\n",
    "\n",
    "# Create a writer\n",
    "local_out_path = f\"/tmp/output_{RUN_ID}.xlsx\"  # Use a local path\n",
    "\n",
    "\n",
    "# Define the path to your Excel file\n",
    "excel_file_path = \"/Volumes/workspace/default/configuration/Output_Mapping.xlsx\"\n",
    "\n",
    "# Read the Excel file \n",
    "df = pd.read_excel(\n",
    "    excel_file_path, \n",
    "    sheet_name='Output mapping', \n",
    "    usecols=\"C:K\", \n",
    "    skiprows=11, \n",
    "    nrows=52\n",
    ") \n",
    "# Read range 'Output mapping'!C12:K63\n",
    "\n",
    "# Rename columns to remove invalid characters \"\" ,;{}()\\n\\t=\"\"\n",
    "df.columns = [\n",
    "    str(column).replace(\" \", \"\")\n",
    "               .replace(\",\", \"_\")\n",
    "               .replace(\";\", \"_\")\n",
    "               .replace(\"(\", \"_\")\n",
    "               .replace(\")\", \"_\")\n",
    "               .replace(\"\\n\", \"_\")\n",
    "    for column in df.columns\n",
    "]\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Save the DataFrame as a new table in the \"default\" schema\n",
    "spark_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"default.Output_Mapping2\")\n",
    "\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Read in the tables as data frames\n",
    "output_mapping_df = spark.table(\"default.Output_Mapping2\")\n",
    "data_id_updated_df = spark.table(\"default.data_id_updated\")\n",
    "aggregation_tree_df = spark.table(\"default.aggregation_tree_market_enriched\")\n",
    "\n",
    "# Step 2: Create a new data frame with enriched values\n",
    "# Create a mapping dictionary for aggregation_tree_market_enriched\n",
    "aggregation_dict = {row[\"_NODE_ID\"]: row[\"VALUE\"] for row in aggregation_tree_df.collect()}\n",
    "# Create a mapping dictionary for data_id_updated\n",
    "data_id_dict = {row[\"DATA_ID\"]: row[\"VALUE\"] for row in data_id_updated_df.collect()}\n",
    "\n",
    "# Combine both dictionaries\n",
    "combined_dict = {**aggregation_dict, **data_id_dict}\n",
    "\n",
    "# Create a UDF to replace values based on the mapping logic\n",
    "replace_udf = F.udf(lambda value: combined_dict.get(value, value), StringType())\n",
    "\n",
    "# Apply the UDF to each relevant column\n",
    "for col_name in [f\"C00{i}\" for i in range(20, 81, 10)]:  # C0020, C0040, C0060, C0080\n",
    "    output_mapping_df = output_mapping_df.withColumn(col_name, replace_udf(F.col(col_name)))\n",
    "\n",
    "# Save the Spark DataFrame as a new table in the \"default\" schema\n",
    "output_mapping_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"default.Output_Enriched2\")\n",
    "\n",
    "\n",
    "\n",
    "# Use the openpyxl engine directly\n",
    "with pd.ExcelWriter(local_out_path, engine='openpyxl') as writer:\n",
    "    writer.book = template\n",
    "    # Write dataframe to excel using template and save in output path\n",
    "    Output_Enriched2_df.toPandas().to_excel(writer, sheet_name='Output mapping', startrow=12, startcol=2, index=False, header=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
